{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Unfair Bias in Machine Learning\n",
    "\n",
    "AI can embed human and societal bias and be then deployed at scale. Many algorithms are now being reexamined due to illegal bias. So how do you remove bias & discrimination in the machine learning pipeline? In this workshop you will learn the debiasing techniques that can be implemented by using the open source toolkit [AI Fairness 360](https://github.com/IBM/AIF360). \n",
    " \n",
    "**AI Fairness 360** (AIF360) is an extensible, open source toolkit for measuring, understanding, and removing AI bias. It contains the most widely used bias metrics, bias mitigation algorithms, and metric explainers from the top AI fairness researchers across industry & academia. \n",
    "\n",
    "In this notebook you will: \n",
    "* apply a practical use case of bias measurement & mitigation\n",
    "* measure bias in data & models \n",
    "* apply the fairness algorithms to reduce bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install aif360  and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "\n",
    "# data exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# aif360 data, metrics and algorithms\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aif360_location = !python -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\"\n",
    "import os\n",
    "install_loc = os.path.join(aif360_location[0], \"aif360/data/raw/german/\")\n",
    "%cd $install_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget ftp://ftp.ics.uci.edu/pub/machine-learning-databases/statlog/german/german.data\n",
    "!wget ftp://ftp.ics.uci.edu/pub/machine-learning-databases/statlog/german/german.doc\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_german = GermanDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIF360 data format\n",
    "\n",
    "All variables of this dataset are described in the [documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.GermanDataset.html) with more details in the description of the [`StandardDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StandardDataset.html). In short, the dataset class contains a numpy array or pandas DataFrame with several variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_german.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'labels: {dataset_german.label_names}')\n",
    "print(f'protected attributes: {dataset_german.protected_attribute_names}')\n",
    "print(f'number of features: {len(dataset_german.feature_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore with pandas\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>If you are new to pandas read this <a href=\"https://developer.ibm.com/technologies/data-science/series/learning-path-data-analysis-using-python/\">practical introduction</a> for a quick overview.<br>\n",
    "</div>\n",
    "\n",
    "Convert the data to a `features` DataFrame and `labels` Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(dataset_german.features, columns=dataset_german.feature_names)\n",
    "labels = pd.Series(dataset_german.labels.ravel(), name=dataset_german.label_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe().transpose().head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the distribution of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,18)\n",
    "\n",
    "features.hist();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features are binary, but a few are continuous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[['credit_amount','month','number_of_credits']]. \\\n",
    "        plot(subplots=True, \\\n",
    "             kind='hist', \\\n",
    "             layout=(2, 2),\n",
    "             sharex=False, \\\n",
    "             figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also add the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fig, axs] = plt.subplots(2, 2, figsize=(14,8))\n",
    "fig.add_subplot(2, 2, 1)\n",
    "features['credit_amount'].plot(kind='hist',bins=12);\n",
    "fig.add_subplot(2, 2, 2)\n",
    "features['month'].plot(kind='hist',bins=12);\n",
    "fig.add_subplot(2, 2, 3)\n",
    "features['number_of_credits'].plot(kind='hist',bins=12);\n",
    "fig.add_subplot(2, 2, 4)\n",
    "labels.plot(kind='hist',bins=12);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore bias in the data\n",
    "\n",
    "Bias could occur based on age or sex in this dataset. \n",
    "\n",
    "* set the protected attribute to be `age`, `age >=25` is considered privileged\n",
    "* this dataset also contains protected attribute for `sex` that are not consider in this evaluation\n",
    "* split the original dataset into training and testing datasets\n",
    "* set two variables for the privileged (1) and unprivileged (0) values for the age attribute. These are key inputs for detecting and mitigating bias\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    " <b>OPTIONAL EXERCISE</b> <br/> \n",
    " To explore the gender bias in the data, edit the below code to use `sex` as the protected attribute and assign new privileged and unprivileged groups.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_german = GermanDataset(protected_attribute_names=['age'],\n",
    "                    privileged_classes=[lambda x: x >= 25],      \n",
    "                    features_to_drop=['personal_status', 'sex']) \n",
    "\n",
    "dataset_german_train, dataset_german_test = dataset_german.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics based on a single `BinaryLabelDataset`\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>Read <a href=\"https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html\">the documentation</a> for a full overview of this class and a list of all bias metrics. <a href=\"http://aif360.mybluemix.net/data\">This demo</a> provides definitions of the metrics as well.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_german_train = BinaryLabelDatasetMetric(dataset_german_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "metric_german_test = BinaryLabelDatasetMetric(dataset_german_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(metric_german_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring bias metrics\n",
    "* `mean_difference`: alias of `statistical_parity_difference` \n",
    "    * Difference of the rate of favorable outcomes received by the unprivileged group to the privileged group. \n",
    "    * A negative value indicates less favorable outcomes for the unprivileged groups\n",
    "    * The ideal value of this metric is 0\n",
    "    * Fairness for this metric is between -0.1 and 0.1\n",
    "\n",
    "* `disparate_impact`: ratio of rate of favorable outcome for the unprivileged group to that of the privileged group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"mean_difference = %f\" % metric_german_train.mean_difference())\n",
    "print(\"disparate_impact = %f\" % metric_german_train.disparate_impact())\n",
    "print(\"consistency = %f\" % metric_german_train.consistency())\n",
    "print(\"base_rate = %f\" % metric_german_train.base_rate())\n",
    "print(\"num_negatives = %f\" % metric_german_train.num_negatives())\n",
    "print(\"num_positives = %f\" % metric_german_train.num_positives())\n",
    "print(\"smoothed_empirical_differential_fairness = %f\" % metric_german_train.smoothed_empirical_differential_fairness())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select and transform features to build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification problem\n",
    "\n",
    "From the above\n",
    "- mostly binary data, a few with a few classes and two continuous. \n",
    "- label is binary: 0 or 1 is what will be predicted, so this will be a **binary classification**\n",
    "\n",
    "Some of the model options:\n",
    "- logistical regression\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Bayesian networks\n",
    "- Support vector machines\n",
    "- Neural networks\n",
    "- Logistic regression\n",
    "\n",
    "### Scale and normalise features\n",
    "\n",
    "- one-hot encoding for multiple classes\n",
    "- features need to be standardised, from same distribution\n",
    "- no missing values\n",
    "- ...\n",
    "\n",
    "[StandardScaler](https://scikit-learn.org/stable/modules/preprocessing.html) - \n",
    "*Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. `StandardScaler` implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set.*\n",
    "\n",
    "aif360 format can be used with scikitlearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scale_german = StandardScaler().fit(dataset_german_train.features)\n",
    "\n",
    "X_train = scale_german.transform(dataset_german_train.features)\n",
    "y_train = dataset_german_train.labels.ravel()\n",
    "w_train = dataset_german_train.instance_weights.ravel()\n",
    "\n",
    "X_test = scale_german.transform(dataset_german_test.features)\n",
    "y_test = dataset_german_test.labels.ravel()\n",
    "w_test = dataset_german_test.instance_weights.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the data look like now?\n",
    "plt.rcParams[\"figure.figsize\"] = (18,18)\n",
    "\n",
    "scaled_features = pd.DataFrame(X_train, columns=dataset_german.feature_names)\n",
    "\n",
    "scaled_features.hist();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build models\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>If you are new to scikit-learn read this <a href=\"https://developer.ibm.com/series/learning-path-machine-learning-for-developers/\">practical introduction</a> for a quick overview.<br>\n",
    "</div>\n",
    "    \n",
    "### Train on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "\n",
    "# create an instance of the model\n",
    "lmod = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "lmod.fit(X_train, y_train, \n",
    "         sample_weight=dataset_german_train.instance_weights)\n",
    "\n",
    "# calculate predicted labels\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# assign positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_german_train.favorable_label)[0][0]\n",
    "\n",
    "# add predicted labels to predictions dataset\n",
    "dataset_german_train_pred = dataset_german_train.copy()\n",
    "dataset_german_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "score = lmod.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, lmod.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bias by reweighing data\n",
    "\n",
    "**Reweighing** is a preprocessing technique that weights the examples in each (group, label) combination differently to ensure fairness before classification.\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>Read the <a href=\"https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html\">aif360 documentation</a> for a full overview<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "\n",
    "# compute the weights for reweighing the dataset\n",
    "RW.fit(dataset_german_train)\n",
    "\n",
    "# transform the dataset to a new dataset based on the estimated transformation\n",
    "dataset_transf_train = RW.transform(dataset_german_train)\n",
    "dataset_transf_test = RW.transform(dataset_german_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"mean_difference = %f\" % metric_german_train.mean_difference())\n",
    "print(\"disparate_impact = %f\" % metric_german_train.disparate_impact())\n",
    "print(\"consistency = %f\" % metric_german_train.consistency())\n",
    "print(\"base_rate = %f\" % metric_german_train.base_rate())\n",
    "print(\"num_negatives = %f\" % metric_german_train.num_negatives())\n",
    "print(\"num_positives = %f\" % metric_german_train.num_positives())\n",
    "print(\"smoothed_empirical_differential_fairness = %f\" % metric_german_train.smoothed_empirical_differential_fairness())\n",
    "\n",
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                         unprivileged_groups=unprivileged_groups,\n",
    "                                         privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Reweighted training dataset\"))\n",
    "print(\"mean_difference = %f\" % metric_transf_train.mean_difference())\n",
    "print(\"disparate_impact = %f\" % metric_transf_train.disparate_impact())\n",
    "print(\"consistency = %f\" % metric_transf_train.consistency())\n",
    "print(\"base_rate = %f\" % metric_transf_train.base_rate())\n",
    "print(\"num_negatives = %f\" % metric_transf_train.num_negatives())\n",
    "print(\"num_positives = %f\" % metric_transf_train.num_positives())\n",
    "print(\"smoothed_empirical_differential_fairness = %f\" % metric_german_train.smoothed_empirical_differential_fairness())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on reweighted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scale_transf = StandardScaler().fit(dataset_transf_train.features)\n",
    "\n",
    "X_train_transf = scale_transf.transform(dataset_transf_train.features)\n",
    "y_train_transf = dataset_transf_train.labels.ravel()\n",
    "w_train_transf = dataset_transf_train.instance_weights.ravel()\n",
    "\n",
    "X_test_transf = scale_transf.transform(dataset_transf_test.features)\n",
    "y_test_transf = dataset_transf_test.labels.ravel()\n",
    "w_test_transf = dataset_transf_test.instance_weights.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of the model\n",
    "lmod_transf = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "lmod_transf.fit(X_train_transf, y_train_transf, \n",
    "         sample_weight=dataset_transf_train.instance_weights)\n",
    "\n",
    "# calculate predicted labels\n",
    "y_train_pred_transf = lmod_transf.predict(X_train_transf)\n",
    "\n",
    "# assign positive class index\n",
    "pos_ind_transf = np.where(lmod_transf.classes_ == dataset_transf_train.favorable_label)[0][0]\n",
    "\n",
    "# add predicted labels to predictions dataset\n",
    "dataset_transf_train_pred = dataset_transf_train.copy()\n",
    "dataset_transf_train_pred.labels = y_train_pred_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "print(score)\n",
    "score2 = lmod_transf.score(X_test_transf, y_test_transf)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm_transf = metrics.confusion_matrix(y_test_transf, lmod_transf.predict(X_test_transf))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm_transf, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Reweighted model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, lmod.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Original model');\n",
    "\n",
    "# age = 1: > 25s privileged "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is still work in progress\n",
    "\n",
    "## in processing\n",
    "https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_adversarial_debiasing.ipynb\n",
    "https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_reject_option_classification.ipynb\n",
    "    \n",
    "## post processing\n",
    "https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_calibrated_eqodds_postprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost constraint of fnr will optimize generalized false negative rates, that of\n",
    "# fpr will optimize generalized false positive rates, and weighted will optimize\n",
    "# a weighted combination of both\n",
    "cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "#random seed for calibrated equal odds prediction\n",
    "randseed = 12345679\n",
    "\n",
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "cpp = cpp.fit(dataset_orig_valid, dataset_orig_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.6], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#scaler = StandardScaler().fit(dataset_german_train.features)\n",
    "\n",
    "scaler = MinMaxScaler(copy=False)\n",
    "\n",
    "train.features = scaler.fit_transform(dataset_german_train.features)\n",
    "test.features = scaler.fit_transform(dataset_german_test.features)\n",
    "\n",
    "index = dataset_german_test.feature_names.index('age')\n",
    "\n",
    "DIs = []\n",
    "for level in tqdm(np.linspace(0., 1., 11)):\n",
    "    di = DisparateImpactRemover(repair_level=level)\n",
    "    train_repd = di.fit_transform(X_train)\n",
    "    test_repd = di.fit_transform(test)\n",
    "    \n",
    "    X_tr = np.delete(train_repd.features, index, axis=1)\n",
    "    X_te = np.delete(test_repd.features, index, axis=1)\n",
    "    y_tr = train_repd.labels.ravel()\n",
    "    \n",
    "    lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "    lmod.fit(X_tr, y_tr)\n",
    "    \n",
    "    test_repd_pred = test_repd.copy()\n",
    "    test_repd_pred.labels = lmod.predict(X_te)\n",
    "\n",
    "    p = [{protected: 1}]\n",
    "    u = [{protected: 0}]\n",
    "    cm = BinaryLabelDatasetMetric(test_repd_pred, privileged_groups=p, unprivileged_groups=u)\n",
    "    DIs.append(cm.disparate_impact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.feature_names.index(protected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# scale data\n",
    "scale_transf = StandardScaler().fit(dataset_transf_train.features)\n",
    "\n",
    "X_train_transf = scale_transf.transform(dataset_transf_train.features)\n",
    "y_train_transf = dataset_transf_train.labels.ravel()\n",
    "w_train_transf = dataset_transf_train.instance_weights.ravel()\n",
    "\n",
    "X_test_transf = scale_transf.transform(dataset_transf_test.features)\n",
    "y_test_transf = dataset_transf_test.labels.ravel()\n",
    "w_test_transf = dataset_transf_test.instance_weights.ravel()\n",
    "\n",
    "# Logistic regression classifier and predictions\n",
    "\n",
    "# create an instance of the model\n",
    "lmod = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "lmod.fit(X_train, y_train, \n",
    "         sample_weight=dataset_german_train.instance_weights)\n",
    "\n",
    "# calculate predicted labels\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# assign positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_german_train.favorable_label)[0][0]\n",
    "\n",
    "# add predicted labels to predictions dataset\n",
    "dataset_german_train_pred = dataset_german_train.copy()\n",
    "dataset_german_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "Margriet Groenendijk is a Data & AI Developer Advocate for IBM. She develops and presents talks and workshops about data science and AI. She is active in the local developer communities through attending, presenting and organising meetups and conferences. She has a background in climate science where she explored large observational datasets of carbon uptake by forests during her PhD, and global scale weather and climate models as a postdoctoral fellow.\n",
    "\n",
    "Copyright © 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
